{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba2d4ba",
   "metadata": {},
   "source": [
    "# Fine-Tuning de VLM (LayoutLMv3) para Extração de Dados de Notas Fiscais\n",
    "\n",
    "Este notebook demonstra o processo completo de ajuste fino (fine-tuning) de um Modelo de Linguagem e Visão (VLM), especificamente o **LayoutLMv3**, para a tarefa de extração de informações de documentos.\n",
    "\n",
    "**Objetivo:** Treinar um modelo capaz de identificar e extrair entidades específicas (como cabeçalhos, perguntas, respostas) de imagens de formulários.\n",
    "\n",
    "**Dataset de Exemplo:** Usaremos o dataset `FUNSD` como um substituto prático para um dataset de Notas Fiscais. Ele contém imagens de formulários e suas respectivas anotações (palavras, coordenadas e rótulos), que é exatamente a estrutura necessária. **O fluxo de trabalho apresentado aqui é 100% aplicável a um dataset customizado de Notas Fiscais.**\n",
    "\n",
    "**Ambiente:** Recomenda-se executar este notebook no Google Colab com um acelerador de GPU ativado (`Ambiente de execução` > `Alterar o tipo de ambiente de execução` > `GPU`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb32f7",
   "metadata": {},
   "source": [
    "## Passo 0: Instalação das Dependências\n",
    "\n",
    "Primeiro, instalamos todas as bibliotecas necessárias da Hugging Face, além de ferramentas auxiliares para processamento de imagem e avaliação de métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0acc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este bloco instala todas as bibliotecas necessárias da Hugging Face e outras\n",
    "# ferramentas que usaremos para processamento de imagem e avaliação.\n",
    "!pip install -q transformers datasets Pillow seqeval accelerate \"evaluate[seqeval]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce414ba7",
   "metadata": {},
   "source": [
    "## Passo 1: Carregamento e Preparação do Dataset\n",
    "\n",
    "Carregamos o dataset de exemplo e preparamos os rótulos (entidades) que queremos que o modelo aprenda a identificar. Para um projeto real, esta é a etapa onde você carregaria suas próprias imagens e anotações (por exemplo, exportadas do Label Studio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef07ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carregamos o dataset do Hugging Face Hub\n",
    "print(\">>> Carregando o dataset de exemplo (FUNSD)...\")\n",
    "dataset = load_dataset(\"nielsr/funsd\")\n",
    "\n",
    "# Vamos inspecionar os rótulos (labels) do dataset\n",
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"\\nOs rótulos (entidades) neste dataset são: {labels}\")\n",
    "\n",
    "# Criamos mapeamentos de ID para rótulo e vice-versa.\n",
    "# Você fará o mesmo para suas entidades (ex: 'cnpj_emitente', 'valor_total').\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "print(f\"\\nMapeamento id2label: {id2label}\")\n",
    "print(f\"Mapeamento label2id: {label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341cce4",
   "metadata": {},
   "source": [
    "## Passo 2: Pré-processamento dos Dados\n",
    "\n",
    "Esta é uma etapa crucial. Convertemos as imagens, textos e suas coordenadas para o formato exato que o modelo LayoutLMv3 espera como entrada. O `AutoProcessor` da Hugging Face simplifica enormemente este processo, lidando com o OCR implícito, tokenização e alinhamento de rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18886b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# Carregamos o processador do modelo. Ele lida com a tokenização do texto\n",
    "# e o processamento da imagem (OCR implícito e normalização).\n",
    "model_checkpoint = \"microsoft/layoutlmv3-base\"\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint, apply_ocr=True)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Pega as imagens, palavras e caixas delimitadoras\n",
    "    images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
    "    words = examples['words']\n",
    "    boxes = examples['bboxes']\n",
    "    word_labels = examples['ner_tags']\n",
    "\n",
    "    # Usa o processador para tokenizar as palavras e preparar as imagens\n",
    "    encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\n",
    "                               padding=\"max_length\", truncation=True)\n",
    "\n",
    "    return encoded_inputs\n",
    "\n",
    "# Aplicamos a função de pré-processamento a todo o dataset\n",
    "print(\"\\n>>> Iniciando o pré-processamento dos dados...\")\n",
    "processed_train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    features=dataset[\"train\"].features\n",
    ")\n",
    "processed_eval_dataset = dataset[\"test\"].map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"test\"].column_names, \n",
    "    features=dataset[\"test\"].features\n",
    ")\n",
    "\n",
    "# Define o formato para PyTorch, que usaremos para o treinamento\n",
    "processed_train_dataset.set_format(type=\"torch\")\n",
    "processed_eval_dataset.set_format(type=\"torch\")\n",
    "\n",
    "print(\"\\nPré-processamento concluído!\")\n",
    "print(f\"Exemplo de uma amostra processada (chaves): {processed_train_dataset[0].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74c7b0",
   "metadata": {},
   "source": [
    "## Passo 3: Fine-Tuning do Modelo\n",
    "\n",
    "Agora, configuramos e executamos o treinamento. Carregamos o modelo `LayoutLMv3ForTokenClassification` pré-treinado, definimos os parâmetros de treinamento (como taxa de aprendizado, número de épocas) e uma função para calcular as métricas de performance (F1, precisão, recall). O `Trainer` da Hugging Face gerencia todo o ciclo de treinamento para nós."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Carregamos o modelo pré-treinado, especificando nossos rótulos customizados\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Carregamos a métrica 'seqeval' para avaliar a performance na extração de entidades\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Removemos os tokens especiais ([CLS], [SEP], [PAD]) para a avaliação\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Definimos os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"meu-modelo-layoutlmv3-notas-fiscais\",\n",
    "    num_train_epochs=3, # Usamos 3 épocas para um teste rápido e eficaz\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Instanciamos o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Inicia o treinamento! (Isso pode levar de 20 a 40 minutos na GPU do Colab)\n",
    "print(\"\\n>>> Iniciando o Fine-Tuning. Isso levará alguns minutos...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8029d",
   "metadata": {},
   "source": [
    "## Passo 4: Salvando o Modelo Final\n",
    "\n",
    "Após o treinamento, salvamos o melhor modelo e o processador em um diretório local. Esses arquivos contêm tudo o que é necessário para usar o modelo posteriormente para inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n>>> Salvando o modelo treinado localmente...\")\n",
    "final_model_path = \"modelo-notas-fiscais-final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "print(f\"Modelo salvo na pasta '{final_model_path}'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e949ba6",
   "metadata": {},
   "source": [
    "## Passo 5: Inferência e Visualização\n",
    "\n",
    "Esta é a etapa final, onde usamos nosso modelo treinado para extrair informações de uma nova imagem. Criamos uma função que processa a imagem, passa pelo modelo e retorna as entidades detectadas e suas coordenadas. Para um resultado mais claro, visualizamos essas detecções desenhando caixas na imagem original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "print(\"\\n>>> Executando inferência em uma nova imagem...\")\n",
    "\n",
    "# Carrega o modelo e processador salvos\n",
    "model_final = AutoModelForTokenClassification.from_pretrained(final_model_path)\n",
    "processor_final = AutoProcessor.from_pretrained(final_model_path)\n",
    "\n",
    "# Função para realizar a inferência e extrair as entidades\n",
    "def extract_entities(image, model, processor):\n",
    "    # Prepara a imagem\n",
    "    encoding = processor(image, return_tensors=\"pt\")\n",
    "    # Move os tensores para a GPU se disponível\n",
    "    if torch.cuda.is_available():\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.to(model.device)\n",
    "    \n",
    "    # Faz a predição\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    # Pega as predições e as coordenadas\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "    boxes = encoding[\"bbox\"].squeeze().tolist()\n",
    "    \n",
    "    # Mapeia as predições de volta para os rótulos\n",
    "    results = []\n",
    "    for pred_id, box in zip(predictions, boxes):\n",
    "        label = model.config.id2label[pred_id]\n",
    "        if label != 'O': # Ignora tokens que não são entidades ('Other')\n",
    "            results.append({\n",
    "                \"label\": label,\n",
    "                \"box\": [int(c) for c in box] # Converte coordenadas para inteiros\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Pega uma imagem do dataset de teste para simular uma \"nova\" nota fiscal\n",
    "image_to_test = Image.open(dataset[\"test\"][0]['image_path']).convert(\"RGB\")\n",
    "\n",
    "# Extrai as entidades da imagem de teste\n",
    "extracted_data = extract_entities(image_to_test, model_final, processor_final)\n",
    "\n",
    "print(\"\\n--- DADOS EXTRAÍDOS ---\")\n",
    "print(extracted_data)\n",
    "\n",
    "# Visualização dos resultados\n",
    "draw = ImageDraw.Draw(image_to_test)\n",
    "label_color_map = {'QUESTION': 'blue', 'ANSWER': 'green', 'HEADER': 'orange'}\n",
    "\n",
    "for data in extracted_data:\n",
    "    box = data['box']\n",
    "    label = data['label'].split('-')[-1] # Pega o rótulo base (ex: B-ANSWER -> ANSWER)\n",
    "    color = label_color_map.get(label, 'red') # Default para vermelho se não mapeado\n",
    "    draw.rectangle(box, outline=color, width=2)\n",
    "    \n",
    "print(\"\\nExibindo imagem com as entidades detectadas:\")\n",
    "image_to_test.save(\"resultado_inferencia.png\")\n",
    "display(image_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a16db",
   "metadata": {},
   "source": [
    "## Próximos Passos\n",
    "\n",
    "1.  **Crie seu Dataset:** Colete suas imagens de Notas Fiscais e use uma ferramenta como o **Label Studio** para anotar as entidades que deseja extrair (ex: `cnpj_emitente`, `valor_total`, `data_emissao`, etc.).\n",
    "\n",
    "2.  **Adapte o Código:**\n",
    "    - Modifique o **Passo 1** para carregar seus dados customizados.\n",
    "    - Atualize as variáveis `labels`, `id2label`, e `label2id` com suas próprias entidades.\n",
    "\n",
    "3.  **Treine o Modelo:** Execute o notebook com seus dados. O resultado será um modelo especialista em extrair informações das suas Notas Fiscais."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
